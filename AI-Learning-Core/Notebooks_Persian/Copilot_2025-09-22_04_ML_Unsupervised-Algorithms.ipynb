{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f927355a",
   "metadata": {},
   "source": [
    "کاملاً متوجه شدم!  \n",
    "در این نسخه، همه‌ی توضیحات، اصطلاحات، مثال‌ها و تمرین‌ها را دقیقاً به همان سبک و با همان جزئیات که در درس‌ها نوشتی، بدون هیچ خلاصه‌سازی یا نکته‌نویسی، به صورت یک فایل Colab (Markdown قابل تبدیل به ipynb) می‌نویسم.  \n",
    "هر الگوریتم یک بخش کامل و مفصل دارد، با توضیح تئوری، اصطلاحات، مثال کد، نکات و تمرین‌ها، دقیقاً مثل یک درس کامل—not خلاصه یا فهرست‌وار.\n",
    "\n",
    "---\n",
    "\n",
    "```markdown\n",
    "# 📘 خوشه‌بندی در یادگیری ماشین – درس جامع و کاربردی\n",
    "\n",
    "---\n",
    "\n",
    "## درس ۱: الگوریتم K-Means\n",
    "\n",
    "### تعریف یادگیری خوشه‌بندی و جایگاه K-Means\n",
    "\n",
    "خوشه‌بندی (Clustering) یکی از شاخه‌های یادگیری ماشین بدون نظارت (Unsupervised Learning) است که هدف آن گروه‌بندی داده‌ها بر اساس شباهت ویژگی‌هاست.  \n",
    "الگوریتم K-Means یکی از ساده‌ترین و پرکاربردترین روش‌های خوشه‌بندی است که داده‌ها را به K گروه تقسیم می‌کند. هر گروه یا خوشه شامل داده‌هایی است که بیشترین شباهت را به هم دارند و از خوشه‌های دیگر متمایز هستند.\n",
    "\n",
    "### مراحل اجرای الگوریتم K-Means\n",
    "\n",
    "۱. **انتخاب مقدار K:**  \n",
    "   ابتدا باید تعداد خوشه‌ها (K) را مشخص کنیم. این مقدار معمولاً با تحلیل داده‌ها یا روش‌هایی مثل Elbow Method انتخاب می‌شود.\n",
    "\n",
    "۲. **انتخاب تصادفی مراکز اولیه خوشه‌ها (Centroids):**  \n",
    "   الگوریتم به صورت تصادفی K نقطه را به عنوان مراکز اولیه خوشه‌ها انتخاب می‌کند.\n",
    "\n",
    "۳. **اختصاص هر داده به نزدیک‌ترین مرکز:**  \n",
    "   هر داده به خوشه‌ای اختصاص می‌یابد که مرکز آن کمترین فاصله را با داده دارد (معمولاً فاصله اقلیدسی).\n",
    "\n",
    "۴. **محاسبه مراکز جدید:**  \n",
    "   برای هر خوشه، مرکز جدید با میانگین‌گیری از داده‌های آن خوشه محاسبه می‌شود.\n",
    "\n",
    "۵. **تکرار مراحل ۳ و ۴:**  \n",
    "   این فرآیند تا زمانی ادامه پیدا می‌کند که مراکز خوشه‌ها تغییر محسوسی نداشته باشند یا به تعداد مشخصی تکرار برسیم.\n",
    "\n",
    "### اصطلاحات کلیدی\n",
    "\n",
    "- **خوشه (Cluster):** گروهی از داده‌ها با ویژگی‌های مشابه.\n",
    "- **مرکز خوشه (Centroid):** نقطه‌ای که میانگین داده‌های هر خوشه را نشان می‌دهد.\n",
    "- **Inertia:** مجموع فاصله داده‌ها تا مرکز خوشه‌شان؛ معیار کیفیت خوشه‌بندی.\n",
    "- **Iteration:** هر بار اجرای مراحل اختصاص و به‌روزرسانی مراکز.\n",
    "- **Elbow Method:** روشی برای انتخاب مقدار مناسب K با بررسی تغییرات Inertia.\n",
    "\n",
    "### مثال تصویری (مفهومی)\n",
    "\n",
    "```\n",
    "[داده‌ها] → [انتخاب K مرکز] → [اختصاص داده‌ها به نزدیک‌ترین مرکز] → [محاسبه مراکز جدید] → [تکرار]\n",
    "```\n",
    "\n",
    "### پیاده‌سازی با پایتون (Scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# فرض کنید X داده‌های شماست\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "score = silhouette_score(X_scaled, labels)\n",
    "\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "print(\"مرکز خوشه‌ها:\n",
    "\", centers)\n",
    "print(\"امتیاز سیلوئت:\", score)\n",
    "```\n",
    "\n",
    "### نکات مهم و تفاوت‌ها\n",
    "\n",
    "- **انتخاب مقدار K:** مقدار K باید با توجه به ساختار داده‌ها انتخاب شود. اگر K خیلی کم باشد، خوشه‌ها بیش از حد بزرگ و غیرواقعی می‌شوند؛ اگر خیلی زیاد باشد، خوشه‌ها کوچک و پراکنده خواهند شد.\n",
    "- **حساسیت به مقدار اولیه:** انتخاب مراکز اولیه می‌تواند روی نتیجه نهایی تأثیر بگذارد. برای همین از روش‌هایی مثل `k-means++` استفاده می‌شود.\n",
    "- **مقیاس داده‌ها:** چون الگوریتم بر اساس فاصله کار می‌کند، داده‌ها باید نرمال‌سازی شوند.\n",
    "- **K-Means فقط برای خوشه‌های کروی و هم‌اندازه مناسب است.**\n",
    "- **الگوریتم ممکن است در داده‌های با شکل پیچیده یا چگالی متفاوت عملکرد ضعیفی داشته باشد.**\n",
    "\n",
    "### تمرین‌ها\n",
    "\n",
    "- **ex-61:** داده‌ها را با K=3 خوشه‌بندی کن و برچسب هر نمونه را نمایش بده.\n",
    "- **ex-62:** مقدار K را از 2 تا 6 تغییر بده و امتیاز silhouette را برای هر حالت محاسبه کن.\n",
    "- **ex-63:** داده‌ها را قبل و بعد از نرمال‌سازی خوشه‌بندی کن و تفاوت نتایج را تحلیل کن.\n",
    "\n",
    "---\n",
    "\n",
    "## درس ۲: الگوریتم DBSCAN\n",
    "\n",
    "### تعریف و کاربرد\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) یک الگوریتم خوشه‌بندی مبتنی بر چگالی است که می‌تواند خوشه‌هایی با شکل‌های پیچیده را شناسایی کند و داده‌های پرت (Noise) را به طور خودکار جدا کند.  \n",
    "برخلاف K-Means، نیازی به تعیین تعداد خوشه‌ها ندارد و برای داده‌هایی با چگالی متغیر مناسب‌تر است.\n",
    "\n",
    "### مراحل اجرای DBSCAN\n",
    "\n",
    "۱. **انتخاب پارامترها:**  \n",
    "   - `eps`: شعاع همسایگی هر نقطه  \n",
    "   - `min_samples`: حداقل تعداد نقاط برای تشکیل یک خوشه\n",
    "\n",
    "۲. **شناسایی نقاط Core:**  \n",
    "   نقطه‌ای که حداقل `min_samples` نقطه در شعاع `eps` دارد، Core Point است.\n",
    "\n",
    "۳. **گسترش خوشه‌ها:**  \n",
    "   از هر Core Point، نقاط همسایه به خوشه اضافه می‌شوند و این فرآیند تا زمانی که نقاط جدیدی پیدا نشود ادامه می‌یابد.\n",
    "\n",
    "۴. **شناسایی داده‌های پرت:**  \n",
    "   نقاطی که در هیچ خوشه‌ای قرار نمی‌گیرند، به عنوان Noise برچسب می‌خورند.\n",
    "\n",
    "### اصطلاحات کلیدی\n",
    "\n",
    "- **Core Point:** نقطه‌ای با حداقل تعداد همسایه در شعاع eps.\n",
    "- **Border Point:** نقطه‌ای که در شعاع Core Point است اما خودش Core نیست.\n",
    "- **Noise Point:** نقطه‌ای که در هیچ خوشه‌ای قرار نمی‌گیرد.\n",
    "- **eps:** شعاع همسایگی.\n",
    "- **min_samples:** حداقل تعداد نقاط برای تشکیل خوشه.\n",
    "\n",
    "### پیاده‌سازی با پایتون\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(X_scaled)\n",
    "\n",
    "labels = dbscan.labels_\n",
    "score = silhouette_score(X_scaled, labels)\n",
    "\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "print(\"امتیاز سیلوئت:\", score)\n",
    "```\n",
    "\n",
    "### نکات مهم و تفاوت‌ها\n",
    "\n",
    "- **DBSCAN برای داده‌هایی با شکل‌های غیرخطی و چگالی متفاوت مناسب‌تر از K-Means است.**\n",
    "- **داده‌های پرت با برچسب -1 مشخص می‌شوند.**\n",
    "- **انتخاب مناسب eps و min_samples بسیار مهم است و روی نتیجه تأثیر زیادی دارد.**\n",
    "- **در داده‌های با چگالی متغیر، ممکن است عملکرد ضعیف‌تری داشته باشد.**\n",
    "\n",
    "### تمرین‌ها\n",
    "\n",
    "- **ex-64:** داده‌ها را با DBSCAN خوشه‌بندی کن و برچسب هر نمونه را نمایش بده.\n",
    "- **ex-65:** مقدار eps را تغییر بده و تأثیر آن را روی تعداد خوشه‌ها بررسی کن.\n",
    "- **ex-66:** داده‌های پرت را شناسایی کن و تحلیل کن که چرا خارج از خوشه‌ها قرار گرفته‌اند.\n",
    "\n",
    "---\n",
    "\n",
    "## درس ۳: الگوریتم Mean Shift\n",
    "\n",
    "### تعریف و کاربرد\n",
    "\n",
    "Mean Shift یک الگوریتم خوشه‌بندی مبتنی بر چگالی است که مراکز خوشه‌ها را با حرکت دادن نقاط به سمت نواحی با چگالی بیشتر پیدا می‌کند.  \n",
    "این الگوریتم نیازی به تعیین تعداد خوشه‌ها ندارد و برای داده‌هایی با شکل‌های پیچیده مناسب است.\n",
    "\n",
    "### مراحل اجرای Mean Shift\n",
    "\n",
    "۱. **انتخاب bandwidth:**  \n",
    "   شعاعی که برای محاسبه چگالی استفاده می‌شود.\n",
    "\n",
    "۲. **حرکت نقاط به سمت میانگین نقاط اطراف:**  \n",
    "   هر نقطه به سمت میانگین نقاطی که در شعاع bandwidth قرار دارند حرکت می‌کند.\n",
    "\n",
    "۳. **تکرار تا رسیدن به مراکز پایدار:**  \n",
    "   این فرآیند تا زمانی ادامه می‌یابد که مراکز خوشه‌ها تغییر محسوسی نداشته باشند.\n",
    "\n",
    "### اصطلاحات کلیدی\n",
    "\n",
    "- **Bandwidth:** شعاع برای محاسبه چگالی.\n",
    "- **Kernel:** تابع وزن‌دهی به نقاط اطراف.\n",
    "\n",
    "### پیاده‌سازی با پایتون\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "meanshift = MeanShift()\n",
    "meanshift.fit(X_scaled)\n",
    "\n",
    "labels = meanshift.labels_\n",
    "centers = meanshift.cluster_centers_\n",
    "\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "print(\"مرکز خوشه‌ها:\n",
    "\", centers)\n",
    "```\n",
    "\n",
    "### نکات مهم و تفاوت‌ها\n",
    "\n",
    "- **بدون نیاز به تعیین تعداد خوشه‌ها.**\n",
    "- **انتخاب bandwidth تأثیر زیادی روی نتیجه دارد.**\n",
    "- **مناسب برای داده‌هایی با شکل‌های پیچیده و چگالی متفاوت.**\n",
    "\n",
    "### تمرین‌ها\n",
    "\n",
    "- **ex-67:** داده‌ها را با Mean Shift خوشه‌بندی کن و مراکز خوشه‌ها را نمایش بده.\n",
    "- **ex-68:** تأثیر تغییر bandwidth را بررسی کن.\n",
    "\n",
    "---\n",
    "\n",
    "## درس ۴: الگوریتم Agglomerative Clustering (خوشه‌بندی سلسله‌مراتبی)\n",
    "\n",
    "### تعریف و کاربرد\n",
    "\n",
    "خوشه‌بندی سلسله‌مراتبی (Hierarchical Clustering) روشی است که داده‌ها را به صورت سلسله‌مراتبی گروه‌بندی می‌کند.  \n",
    "در روش Agglomerative (پایین به بالا)، ابتدا هر داده یک خوشه جداگانه است و خوشه‌ها به تدریج با هم ترکیب می‌شوند تا به تعداد مشخصی برسیم.\n",
    "\n",
    "### مراحل اجرای Agglomerative Clustering\n",
    "\n",
    "۱. **هر داده یک خوشه:**  \n",
    "   در ابتدا هر داده یک خوشه مجزا است.\n",
    "\n",
    "۲. **ترکیب نزدیک‌ترین خوشه‌ها:**  \n",
    "   در هر مرحله، دو خوشه‌ای که کمترین فاصله را دارند با هم ترکیب می‌شوند.\n",
    "\n",
    "۳. **تکرار تا رسیدن به تعداد خوشه‌های مورد نظر:**  \n",
    "   این فرآیند تا زمانی ادامه می‌یابد که به تعداد خوشه‌های دلخواه برسیم.\n",
    "\n",
    "### اصطلاحات کلیدی\n",
    "\n",
    "- **Linkage:** نحوه محاسبه فاصله بین خوشه‌ها (single, complete, average, ward).\n",
    "- **Affinity:** معیار فاصله (اقلیدسی و ...).\n",
    "- **Dendrogram:** نمودار درختی برای نمایش ساختار سلسله‌مراتبی خوشه‌ها.\n",
    "\n",
    "### پیاده‌سازی با پایتون\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels = agglo.fit_predict(X)\n",
    "\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "```\n",
    "\n",
    "### نکات مهم و تفاوت‌ها\n",
    "\n",
    "- **مناسب برای داده‌هایی با ساختار سلسله‌مراتبی.**\n",
    "- **قابل نمایش با دندروگرام.**\n",
    "- **نیاز به تعیین تعداد خوشه‌ها.**\n",
    "- **انتخاب linkage مناسب روی نتیجه تأثیر دارد.**\n",
    "\n",
    "### تمرین‌ها\n",
    "\n",
    "- **ex-69:** داده‌ها را با Agglomerative Clustering خوشه‌بندی کن.\n",
    "- **ex-70:** تفاوت بین linkage‌های مختلف را بررسی کن.\n",
    "\n",
    "---\n",
    "\n",
    "## درس ۵: الگوریتم Gaussian Mixture Model (GMM)\n",
    "\n",
    "### تعریف و کاربرد\n",
    "\n",
    "GMM یک مدل آماری است که فرض می‌کند داده‌ها از ترکیب چند توزیع نرمال (Gaussian) تشکیل شده‌اند.  \n",
    "برخلاف K-Means که هر داده را به یک خوشه اختصاص می‌دهد (خوشه‌بندی سخت)، GMM احتمال تعلق هر داده به هر خوشه را محاسبه می‌کند (خوشه‌بندی نرم).\n",
    "\n",
    "### مراحل اجرای GMM\n",
    "\n",
    "۱. **تعیین تعداد خوشه‌ها (n_components):**  \n",
    "   تعداد توزیع‌های نرمالی که باید مدل شود.\n",
    "\n",
    "۲. **مقداردهی اولیه پارامترها:**  \n",
    "   پارامترهای اولیه میانگین، واریانس و وزن هر خوشه تعیین می‌شود.\n",
    "\n",
    "۳. **تکرار مراحل E و M (Expectation-Maximization):**  \n",
    "   - **E:** محاسبه احتمال تعلق هر داده به هر خوشه  \n",
    "   - **M:** به‌روزرسانی پارامترهای مدل بر اساس این احتمالات  \n",
    "   این فرآیند تا همگرایی ادامه می‌یابد.\n",
    "\n",
    "### اصطلاحات کلیدی\n",
    "\n",
    "- **Expectation-Maximization (EM):** الگوریتم آموزش مدل.\n",
    "- **Covariance Type:** نوع ماتریس کوواریانس (full, tied, diag, spherical).\n",
    "- **Soft Clustering:** هر داده می‌تواند به چند خوشه با احتمال متفاوت تعلق داشته باشد.\n",
    "\n",
    "### پیاده‌سازی با پایتون\n",
    "\n",
    "```python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "```\n",
    "\n",
    "### نکات مهم و تفاوت‌ها\n",
    "\n",
    "- **خوشه‌بندی نرم (Soft Clustering):** هر داده می‌تواند به چند خوشه با احتمال متفاوت تعلق داشته باشد.\n",
    "- **مناسب برای داده‌هایی با توزیع آماری و شکل‌های پیچیده.**\n",
    "- **قابل استفاده برای مدل‌سازی داده‌های پیچیده و شبیه‌سازی داده‌ها.**\n",
    "\n",
    "### تمرین‌ها\n",
    "\n",
    "- **ex-71:** داده‌ها را با GMM خوشه‌بندی کن و برچسب‌ها را نمایش بده.\n",
    "- **ex-72:** تفاوت بین خوشه‌بندی سخت (K-Means) و نرم (GMM) را تحلیل کن.\n",
    "\n",
    "---\n",
    "\n",
    "## درس ۶: الگوریتم‌های تکمیلی (اختیاری)\n",
    "\n",
    "### Spectral Clustering\n",
    "\n",
    "Spectral Clustering یک الگوریتم خوشه‌بندی بر پایه گراف و ماتریس شباهت است که برای داده‌هایی با ساختار پیچیده مناسب است.  \n",
    "در این روش، داده‌ها به یک فضای جدید (فضای ویژه) نگاشته می‌شوند و سپس خوشه‌بندی (معمولاً با K-Means) روی آن انجام می‌شود.\n",
    "\n",
    "#### پیاده‌سازی با پایتون\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "labels = spectral.fit_predict(X)\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "```\n",
    "\n",
    "### Birch\n",
    "\n",
    "Birch (Balanced Iterative Reducing and Clustering using Hierarchies) الگوریتمی سریع و مقیاس‌پذیر برای داده‌های حجیم است که خوشه‌بندی سلسله‌مراتبی را با ساختار درختی انجام می‌دهد.\n",
    "\n",
    "#### پیاده‌سازی با پایتون\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch = Birch(n_clusters=3)\n",
    "labels = birch.fit_predict(X)\n",
    "print(\"برچسب خوشه‌ها:\", labels)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "این ساختار دقیقاً مطابق با سبک درس‌های قبلی و بدون حذف هیچ توضیح یا جزئیاتی است.  \n",
    "تمام بخش‌ها، اصطلاحات، مثال‌ها و تمرین‌ها به صورت کامل و قابل اجرا در Colab/Jupyter آماده شده‌اند.\n",
    "\n",
    "آیا این ساختار دقیقاً همان چیزی است که مدنظرت بود؟  \n",
    "اگر نیاز به اضافه کردن مثال داده یا توضیح بیشتر داری، بگو تا تکمیل کنم!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
